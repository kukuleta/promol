# -*- coding: utf-8 -*-
"""datasets.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1SYKff3gii4cKbahFsHLa1e-X8zAzaMUO
"""

from rdkit.Chem import AllChem as Chem

import json
import shutil
import hashlib
from pathlib import Path
from functools import partial, reduce

import torch
from torch import nn
from torch.utils.data import Dataset
from torch.nn import functional as F
from torchvision.datasets.utils import download_url, check_integrity

import numpy as np
import pandas as pd

from rdkit.Chem import AllChem as Chem

"""#### Base-dataset definitions"""

def get_md5(file):
    md5o = hashlib.md5()
    with open(file, 'rb') as f:
        for chunk in iter(lambda: f.read(1024 * 1024), b''):
            md5o.update(chunk)
    md5c = md5o.hexdigest()
    return md5c

class BiosnapMiner(Dataset):

      base_folder = 'BiosnapMiner'
      url = "http://snap.stanford.edu/biodata/datasets/10002/files/ChG-Miner_miner-chem-gene.tsv.gz"
      filename = "ChG-Miner.tsv.gz"
      tgz_md5 = "f7e1eab1a5e8cda04728a8067a7d2f3e"
      processed_file_name = "ChGMiner.csv"
      mask_name = "ChGMiner_target.csv"
      tgz_after_md5 = "cf2fbd1c0c920260b0a20e93f9ee27ff"
      seperator = "\t"


      def __init__(self,
                   root,
                   train=True,
                   transform=None,
                   target_transform=None,
                   ):
        
          self.root = Path(root).expanduser()
          self.train = train
          self.transform = transform
          self.target_transform = target_transform

          if check_integrity(self.root / self.base_folder / self.processed_file_name, self.tgz_after_md5):
             self.data = pd.read_csv(self.root / self.base_folder / self.processed_file_name)

          else:
             self.download()
             self.data = pd.read_csv(self.root / self.base_folder / self.processed_file_name)

      def download(self):

          import gzip

          root = self.root
          download_url(self.url, root, self.filename, self.tgz_md5)#, self.tgz_md5)

          (self.root / self.base_folder).mkdir(exist_ok=True)

          with gzip.open(root / self.filename, "r") as f_in:
               data = pd.read_csv(f_in, sep=self.seperator)
               data.to_csv(root / self.base_folder / self.processed_file_name, index=False)

          self.filename = self.filename.rsplit(".", 1)[0]

class BiosnapDecagon(BiosnapMiner):

      base_folder = 'BiosnapDecagon'
      url = "http://snap.stanford.edu/biodata/datasets/10015/files/ChG-TargetDecagon_targets.csv.gz"
      filename = "BiosnapDecagon.csv.gz"
      tgz_md5 = "f0319ab14618dc58d2099d207b350ba1"
      processed_file_name = "BiosnapDecagon.csv"
      tgz_after_md5 = "cf2fbd1c0c920260b0a20e93f9ee27ff"

      def __init__(self,
                   root,
                   train=True,
                   transform=None,
                   target_transform=None,
                   ):
        
          super(BiosnapDecagon, self).__init__(root,
                                               train,
                                               transform,
                                               target_transform)
          
          self.data = self.data.iloc[:, 0].astype(str).str.split(",")
          self.data = pd.DataFrame(self.data.to_list(), columns=['Drug','Gene'])

class BindingDB(Dataset):

      base_folder = 'BindingDB'
      url = "https://www.bindingdb.org/bind/downloads/BindingDB_All_2021m4.tsv.zip"
      filename = "BindingDB_All.tsv.zip"
      tgz_md5 = "58375c36233320e4dbbb5ce3c467b7e6"
      processed_file_name = "BindingDB_All_processed.csv"
      mask_name = "BindingDB_All_processed_target.csv"
      tgz_after_md5 = "594723bd4aba69dada4533d87a6a868a"


      def __init__(self,
                   root,
                   train=True,
                   transform=None,
                   target_transform=None,
                   interaction_idx="Kd (nM)",
                   log=True,
                   binary=True,
                   threshold=30,
                   cache=True,
                   ):
        
          self.root = Path(root).expanduser()
          self.train = train
          self.transform = transform
          self.target_transform = target_transform
          self.binary = binary
          self.log = log
          self.threshold = threshold

          self.preferred_affinity_identifier = interaction_idx

          if cache: 
             if check_integrity(self.root / self.base_folder / self.processed_file_name, self.tgz_after_md5):
                  self.data = pd.read_csv(self.root / self.base_folder / self.processed_file_name)

          else:
             self.data = self.generate_affinity_matrix()

          if binary:
             self.mask = (self.data[self.preferred_affinity_identifier] < self.threshold) * 1


      def download(self):

          from zipfile import ZipFile

          root = self.root
          download_url(self.url, root, self.filename, self.tgz_md5)

          
          with ZipFile(root / self.filename, "r") as f_in:
               f_in.extractall(self.root / self.base_folder)

          self.filename = self.filename.rsplit(".", 1)[0]

      def generate_affinity_matrix(self):

          self.download()
      
          NUM_PROTEIN_CHAIN_IDENTIFIER = '`%s`' % "Number of Protein Chains in Target (>1 implies a multichain complex)"
          SMILE_IDENTIFIER = '`%s`' % "Ligand SMILES"
          PUBCHEM_CID_IDENTIFIER = '`%s`' % "PubChem CID"
          UNIPROT_ID_IDENTIFIER = '`%s`' % "UniProt (SwissProt) Primary ID of Target Chain"
          CHI_IDENTIFIER = '`%s`' % "Ligand InChI"
          AFFINITY_IDENTIFIER = '`%s`' % self.preferred_affinity_identifier


          generate_query_expansion = lambda operator, conditions: "(%s)" % (f" {operator} ").join(["  ".join(condition) \
                                                                                                   for condition in conditions])

          conditions = [(AFFINITY_IDENTIFIER, '==', AFFINITY_IDENTIFIER),
                        (NUM_PROTEIN_CHAIN_IDENTIFIER, "==", "1"),
                        (SMILE_IDENTIFIER, '==', SMILE_IDENTIFIER),
                        (CHI_IDENTIFIER, '==', CHI_IDENTIFIER)]

          query = generate_query_expansion("&", conditions) \
                        + " & " + \
                       generate_query_expansion("|", [(PUBCHEM_CID_IDENTIFIER, '==', PUBCHEM_CID_IDENTIFIER),
                                                     (UNIPROT_ID_IDENTIFIER, '==', UNIPROT_ID_IDENTIFIER)])
                      
          data = pd.read_csv(self.root / self.base_folder / self.filename, sep="\t", error_bad_lines=False, nrows=5000)
          data = data.query(query)
          data[self.preferred_affinity_identifier] = data[self.preferred_affinity_identifier]\
                                                         .astype(str)\
                                                         .str.replace(">", "")\
                                                         .str.replace("<", "")\
                                                         .astype(float)

          data = data.query(f"{AFFINITY_IDENTIFIER} < 1e7")

          necessary_columns = ['BindingDB Reactant_set_id', 'Ligand InChI', 'Ligand SMILES',\
                              'PubChem CID', 'UniProt (SwissProt) Primary ID of Target Chain',\
                              'BindingDB Target Chain  Sequence', self.preferred_affinity_identifier]

          renamed_column_names = ["ID", "InChI", "SMILES", "PubChem_ID", "UniProt_ID",
                                  "BindingDB Target Chain  Sequence", "Target Sequence"]

          data = data.loc[:, data.columns.isin(necessary_columns)]
                     
          data.rename(columns=dict(zip(necessary_columns[:-1],
                                              renamed_column_names)),
                      inplace=True)
        

          data.to_csv(self.root / self.base_folder / self.processed_file_name, index=False)
          print(get_md5(self.root / self.base_folder / self.processed_file_name))

          return data


      def __getitem__(self, idx):
          return self.data[idx], self.mask[idx]


      def __len__(self):
          return len(self.data)

class KIBA(Dataset):

      base_folder = 'KIBA'
      url = "https://drive.google.com/uc?export=download&id=1fb3ZI-3_865OuRMWNMzLPnbLm9CktM44"
      filename = "kiba.zip"
      tgz_md5 = "eb645f8960e87adc20b8993641b1b378"
      processed_file_name = "kiba.csv"
      tgz_after_md5 = "7b337c69c002d1c1d3d231d6281ef9b2"

      def __init__(self,
                   root,
                   train=True,
                   transform=None,
                   target_transform=None,
                   log=True,
                   binary=True,
                   threshold=9,
                   ):
        
          self.root = Path(root).expanduser()
          self.train = train
          self.transform = transform
          self.target_transform = target_transform
          self.log = log
          self.binary = binary
          self.threshold = threshold
          self.preferred_affinity_identifier = "Kd (nM)"
          
          if check_integrity(self.root / self.base_folder / self.processed_file_name, self.tgz_after_md5):
             self.data = pd.read_csv(self.root / self.base_folder / self.processed_file_name)

          else:
             self.data = self.generate_affinity_matrix()

          if binary:
             self.mask = (self.data[self.preferred_affinity_identifier] < self.threshold) * 1


      def download(self):

          from zipfile import ZipFile

          root = self.root
          download_url(self.url, root, self.filename, self.tgz_md5)

          with ZipFile(root / self.filename, "r") as f_in:
               f_in.extractall(root)
               
      def generate_affinity_matrix(self):
          
          self.download()

          files = {"smile": "SMILES.txt",
                   "sequence": "target_seq.txt",
                   "affinity": "affinity.txt"}

          with open(self.root / Path("KIBA") / files["smile"], "rb") as smiles, \
               open(self.root / Path("KIBA") / files["sequence"], "rb") as sequences:

              cid2smile = json.load(smiles)
              name2sequence = json.load(sequences)
              

          affinity_scores = pd.read_csv(self.root / Path("KIBA") / files["affinity"], header=None, sep='\t').iloc[:, :-1]

          affinity_scores.index = cid2smile.keys()
          affinity_scores.columns = name2sequence.keys()

          affinity_scores.columns.name = "GeneName"
          affinity_scores.index.name = "PubChemCID"
          
          affinity_scores = affinity_scores.unstack().to_frame(name=self.preferred_affinity_identifier).reset_index()

          affinity_scores = affinity_scores.loc[~affinity_scores[self.preferred_affinity_identifier].isna(), :]

          artifacts = ["__MACOSX"]

          for dir in artifacts:
              path = Path(self.root / dir)
              if path.exists():
                  shutil.rmtree(path)

          affinity_scores.to_csv(self.root / self.base_folder / self.processed_file_name)

          return affinity_scores

class Davis(Dataset):

      base_folder = 'DAVIS'
      url = "https://drive.google.com/uc?export=download&id=14h-0YyHN8lxuc0KV3whsaSaA-4KSmiVN"
      filename = "davis.zip"
      tgz_md5 = "f3ed7e1ad274c95cceea0b20a5ae9568"
      processed_file_name = "davis.csv"
      mask_name = "davis_target.csv"
      tgz_after_md5 = "fa1dd2894d110abe251b204cedb317bd"

      def __init__(self,
                   root,
                   train=True,
                   transform=None,
                   target_transform=None,
                   log=True,
                   binary=True,
                   threshold=30,
                   ):
        
          self.root = Path(root).expanduser()
          self.train = train
          self.transform = transform
          self.target_transform = target_transform
          self.log = log
          self.binary = binary
          self.threshold = threshold
          self.preferred_affinity_identifier = "Kd (nM)"
          
          if check_integrity(self.root / self.base_folder / self.processed_file_name, self.tgz_after_md5):
             self.data = pd.read_csv(self.root / self.base_folder / self.processed_file_name)

          else:
             self.data = self.generate_affinity_matrix()

          if binary:
             self.mask = (self.data[self.preferred_affinity_identifier] < self.threshold) * 1

      def download(self):

          from zipfile import ZipFile

          download_url(self.url, self.root, self.filename, self.tgz_md5)

          with ZipFile(self.root / self.filename, "r") as f_in:
               f_in.extractall(self.root)
               
      def generate_affinity_matrix(self):
          
          self.download()

          davis_dir = self.root / self.base_folder
          
          davis_dir.mkdir(exist_ok=True)

          with open(self.root / Path("DAVIS") / "SMILES.txt", "rb") as smiles, \
               open(self.root / Path("DAVIS") / "target_seq.txt", "rb") as sequences:

              cid2smile = json.load(smiles)
              name2sequence = json.load(sequences)


          affinity_scores = pd.read_csv(self.root / Path("DAVIS") / "affinity.txt", header=None, sep=" ")

          affinity_scores.index = cid2smile.keys()
          affinity_scores.columns = name2sequence.keys()

          affinity_scores.columns.name = "GeneName"
          affinity_scores.index.name = "PubChemCID"

          affinity_scores = affinity_scores.unstack().to_frame(name=self.preferred_affinity_identifier).reset_index()


          affinity_scores.to_csv(self.root / self.base_folder / self.processed_file_name, index=False)

          artifacts = ["__MACOSX"]

          for dir in artifacts:
              path = Path(self.root / dir)
              if path.exists():
                  shutil.rmtree(path)

          return affinity_scores

"""#### Sub-dataset definitions"""

class MolTransDataset(Dataset):
    pass

class DeepDTADataset(Dataset):

    MAX_SMILE_SEQUENCE_LENGTH = 100
    MAX_TARGET_SEQUENCE_LENGTH = 1000

    AMINO_ALPHABET = ['?', 'A', 'C', 'B', 'E',
                      'D', 'G', 'L', 'O', 'U',
                      'F', 'I', 'H', 'K', 'M',
                      'N', 'Q', 'P', 'S', 'R', 
                      'W', 'V', 'Y', 'X', 'Z',
                      'T']

    SMILE_ALPHABET = ['?', '#', '%', ')', '(', '+',
                      '.', '1', '0', '3', '2', '5',
                      '7', '6', '9', '8', '=', 'A', 
                      'B', 'E', 'D', 'G', 'F', 'I',
                      'H', 'K', 'M', 'L', 'O', 'N',
                      'S', 'R', 'U', 'T', 'W', 'V',
                      'Y', '[', 'Z', ']', '_', 'a',
                      'c', 'b', 'e', 'd', 'g', 'f', 
                      'h', 'm', 'l', 'o', 'n', 's',
                      'r', 'u', 't', 'y', 'i', 'P',
                      'C', '4', '-']

    aminochar2idx = dict(zip(AMINO_ALPHABET, range(len(AMINO_ALPHABET))))
    smilechar2idx = dict(zip(SMILE_ALPHABET, range(len(SMILE_ALPHABET))))

    def __init__(self, sequences, smiles, affinities, mode="train"):
      
        self.sequences = sequences
        self.smiles = smiles
        self.affinities = affinities
        self.mode = mode


    def __getitem__(self, idx):
        
        sequence = self.pad_sequence(self.sequences[idx], self.MAX_TARGET_SEQUENCE_LENGTH, partial(self.sequence2idx, index=self.aminochar2idx))
        smile = self.pad_sequence(self.smiles[idx], self.MAX_SMILE_SEQUENCE_LENGTH, partial(self.sequence2idx, index=self.smilechar2idx))

        if self.mode == "train":
            return smile, sequence, self.affinities[idx]

        return smile, sequence
    
    def __len__(self):
        return len(self.affinities)

    def pad_sequence(self, sequence, max_length, index_fn):
    
        if len(sequence) < max_length:
          sequence = F.pad(torch.LongTensor(index_fn(sequence)), pad=(0, max_length - len(sequence)))
        
        else:
          sequence = torch.LongTensor(index_fn(sequence)[:max_length])

        return sequence

    def sequence2idx(self, sequence, index):
        return [index[sequence_character] for sequence_character in sequence]

class DeepConvDTIDataset(Dataset):

    MAX_TARGET_SEQUENCE_LENGTH = 2500

    AMINO_ALPHABET = ['?', 'A', 'C', 'B', 'E',
                      'D', 'G', 'L', 'O', 'U',
                      'F', 'I', 'H', 'K', 'M',
                      'N', 'Q', 'P', 'S', 'R', 
                      'W', 'V', 'Y', 'X', 'Z',
                      'T']


    aminochar2idx = dict(zip(AMINO_ALPHABET, range(len(AMINO_ALPHABET))))

    def sequence2idx(sequence, index):
        return [index[sequence_character] for sequence_character in sequence]

    target2idx = partial(sequence2idx, index=aminochar2idx)
    

    def __init__(self, sequences, smiles, affinities, mode="train"):
      
        self.sequences = sequences
        self.smiles = smiles
        
        self.affinities = affinities
        self.mode = mode

    def pad_sequence(self, sequence, max_length, index_fn):

        if len(sequence) < max_length:
          sequence = F.pad(torch.LongTensor(index_fn(sequence)), pad=(0, max_length - len(sequence)))
        
        else:
          sequence = torch.LongTensor(index_fn(sequence)[:max_length])

        return sequence

    def __getitem__(self, idx):
        
        sequence = self.pad_sequence(self.sequences[idx], self.MAX_TARGET_SEQUENCE_LENGTH, self.target2idx)
        smile = np.array(self.smiles[idx])

        if self.mode == "train":
            return torch.FloatTensor(smile), sequence, self.affinities[idx]

        return torch.FloatTensor(smile), sequence
    
    def __len__(self):
        return len(self.affinities)