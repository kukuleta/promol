{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MolTrans & Base Script Training.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "io1I0WLA0QSl"
      },
      "source": [
        "from IPython.display import clear_output\n",
        "!pip install tqdm subword-nmt\n",
        "\n",
        "clear_output()"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8wKZVVUFy0Bs"
      },
      "source": [
        "import sys\n",
        "\n",
        "!git clone https://github.com/kexinhuang12345/MolTrans.git\n",
        "!mv MolTrans/ESPF ESPF && rm -rf MolTrans/dataset\n",
        "\n",
        "sys.path.append(\"MolTrans\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oYy0pPygxzKg",
        "outputId": "7d1c73b0-02a9-451d-86ef-aee1d3080d13"
      },
      "source": [
        "import copy\n",
        "from pathlib import Path\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "\n",
        "from config import BIN_config_DBPE\n",
        "from models import BIN_Interaction_Flat\n",
        "from stream import BIN_Data_Encoder\n",
        "\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import roc_auc_score, average_precision_score, f1_score, roc_curve, confusion_matrix, \\\n",
        "    precision_score, recall_score, auc\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.autograd import Variable\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "np.random.seed(42)\n",
        "torch.manual_seed(42)  # reproducible torch:2 np:3"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f6fd9bc88d0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L45eEYcKxqgu"
      },
      "source": [
        "dataset = \"BIOSNAP\"\n",
        "dataset_path = Path(\"benchmarks\") / dataset\n",
        "train_indices, val_indices, test_indices = [np.fromfile(str(dataset_path / f\"{dataset_type}_indices.bin\"), dtype=int) for dataset_type in [\"train\", \"val\", \"test\"]]\n",
        "data = pd.read_csv(dataset_path / \"data.csv\").reset_index(drop=True)\n",
        "data[\"Target Sequence\"] = data[\"Target Sequence\"].str.upper()\n",
        "cols = [\"Target Sequence\", \"SMILES\", \"Label\"]\n",
        "\n",
        "training_set = BIN_Data_Encoder(train_indices - train_indices.min(), data.loc[train_indices, \"Label\"].values, data.loc[train_indices, cols].reset_index(drop=True))\n",
        "training_generator = torch.utils.data.DataLoader(training_set, **params)\n",
        "\n",
        "validation_set = BIN_Data_Encoder(val_indices - val_indices.min(), data.loc[val_indices, \"Label\"].values, data.loc[val_indices, cols].reset_index(drop=True))\n",
        "validation_generator = torch.utils.data.DataLoader(validation_set, **params)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lrwlFJhbxfkv"
      },
      "source": [
        "lr = 1e-3\n",
        "batch_size = 16\n",
        "workers = 0\n",
        "epochs = 50\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "params = {'batch_size': batch_size,\n",
        "          'shuffle': True,\n",
        "          'num_workers': workers,\n",
        "          'drop_last': True}\n",
        "\n",
        "\n",
        "model = BIN_Interaction_Flat(**BIN_config_DBPE()).cuda()\n",
        "opt = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "torch.backends.cudnn.benchmark = True\n",
        "\n",
        "loss_history = []"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BH9OuaAJxr80"
      },
      "source": [
        "def test(data_generator, model):\n",
        "    y_pred = []\n",
        "    y_label = []\n",
        "    model.eval()\n",
        "    loss_accumulate = 0.0\n",
        "    count = 0.0\n",
        "    for i, (d, p, d_mask, p_mask, label) in enumerate(data_generator):\n",
        "        score = model(d.long().cuda(), p.long().cuda(), d_mask.long().cuda(), p_mask.long().cuda())\n",
        "\n",
        "        m = torch.nn.Sigmoid()\n",
        "        logits = torch.squeeze(m(score))\n",
        "        loss_fct = torch.nn.BCELoss()\n",
        "\n",
        "        label = Variable(torch.from_numpy(np.array(label)).float()).cuda()\n",
        "\n",
        "        loss = loss_fct(logits, label)\n",
        "\n",
        "        loss_accumulate += loss\n",
        "        count += 1\n",
        "\n",
        "        logits = logits.detach().cpu().numpy()\n",
        "\n",
        "        label_ids = label.to('cpu').numpy()\n",
        "        y_label = y_label + label_ids.flatten().tolist()\n",
        "        y_pred = y_pred + logits.flatten().tolist()\n",
        "\n",
        "    loss = loss_accumulate / count\n",
        "\n",
        "    fpr, tpr, thresholds = roc_curve(y_label, y_pred)\n",
        "\n",
        "    precision = tpr / (tpr + fpr)\n",
        "\n",
        "    f1 = 2 * precision * tpr / (tpr + precision + 0.00001)\n",
        "\n",
        "    thred_optim = thresholds[5:][np.argmax(f1[5:])]\n",
        "\n",
        "    print(\"optimal threshold: \" + str(thred_optim))\n",
        "\n",
        "    y_pred_s = [1 if i else 0 for i in (y_pred >= thred_optim)]\n",
        "\n",
        "    auc_k = metrics.auc(fpr, tpr)\n",
        "    print(\"AUROC:\" + str(auc_k))\n",
        "    print(\"AUPRC: \" + str(average_precision_score(y_label, y_pred)))\n",
        "\n",
        "    cm1 = confusion_matrix(y_label, y_pred_s)\n",
        "    print('Confusion Matrix : \\n', cm1)\n",
        "    print('Recall : ', recall_score(y_label, y_pred_s))\n",
        "    print('Precision : ', precision_score(y_label, y_pred_s))\n",
        "\n",
        "    total1 = sum(sum(cm1))\n",
        "    #####from confusion matrix calculate accuracy\n",
        "    accuracy1 = (cm1[0, 0] + cm1[1, 1]) / total1\n",
        "    print('Accuracy : ', accuracy1)\n",
        "\n",
        "    sensitivity1 = cm1[0, 0] / (cm1[0, 0] + cm1[0, 1])\n",
        "    print('Sensitivity : ', sensitivity1)\n",
        "\n",
        "    specificity1 = cm1[1, 1] / (cm1[1, 0] + cm1[1, 1])\n",
        "    print('Specificity : ', specificity1)\n",
        "\n",
        "    outputs = np.asarray([1 if i else 0 for i in (np.asarray(y_pred) >= 0.5)])\n",
        "    return roc_auc_score(y_label, y_pred), average_precision_score(y_label, y_pred), f1_score(y_label,\n",
        "                                                                                              outputs), y_pred, loss.item()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "47nI3PH8x2IF"
      },
      "source": [
        "max_auc = 0\n",
        "loss_fct = torch.nn.BCELoss()\n",
        "\n",
        "\n",
        "for epo in range(epochs):\n",
        "        model.train()\n",
        "        for i, (d, p, d_mask, p_mask, label) in enumerate(training_generator):\n",
        "            score = model(d.long().cuda(), p.long().cuda(), d_mask.long().cuda(), p_mask.long().cuda())\n",
        "\n",
        "            label = Variable(torch.from_numpy(np.array(label)).float()).cuda()\n",
        "\n",
        "            m = torch.nn.Sigmoid()\n",
        "            n = torch.squeeze(m(score))\n",
        "\n",
        "            loss = loss_fct(n, label)\n",
        "            loss.requires_grad = True\n",
        "            #loss.requires_grad = True \n",
        "            loss_history.append(loss)\n",
        "\n",
        "            opt.zero_grad()\n",
        "            loss.backward()\n",
        "            opt.step()\n",
        "\n",
        "            if (i % 1000 == 0):\n",
        "                print('Training at Epoch ' + str(epo + 1) + ' iteration ' + str(i) + ' with loss ' + str(\n",
        "                    loss.cpu().detach().numpy()))\n",
        "\n",
        "        # every epoch test\n",
        "        with torch.set_grad_enabled(False):\n",
        "            auc, auprc, f1, logits, loss = test(validation_generator, model)\n",
        "            if auc > max_auc:\n",
        "                model_max = copy.deepcopy(model)\n",
        "                max_auc = auc\n",
        "            print('Validation at Epoch ' + str(epo + 1) + ' , AUROC: ' + str(auc) + ' , AUPRC: ' + str(\n",
        "                auprc) + ' , F1: ' + str(f1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QTgWUKz4xsp9"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}